# ğŸ”¬ AUDIT DATA SCIENCE APPROFONDI
## Pipeline PDO - Analyse des DÃ©fauts MÃ©thodologiques et Techniques

---

**Classification** : Critique - Revue de Code Data Science  
**PÃ©rimÃ¨tre** : Feature Engineering, Typage, Formulation MathÃ©matique, Bonnes Pratiques ML  
**Verdict** : ğŸ”´ **NON CONFORME** aux standards industriels et acadÃ©miques actuels

---

# SOMMAIRE

1. [Erreur Critique : Formule Logistique Non Standard](#1-erreur-critique--formule-logistique-non-standard)
2. [Erreur Critique : Intercept Non UtilisÃ©](#2-erreur-critique--intercept-non-utilisÃ©)
3. [ProblÃ¨mes de Typage des Features](#3-problÃ¨mes-de-typage-des-features)
4. [DÃ©fauts de Feature Engineering](#4-dÃ©fauts-de-feature-engineering)
5. [Absence de Standards Data Science](#5-absence-de-standards-data-science)
6. [ProblÃ¨mes d'Encodage CatÃ©goriel](#6-problÃ¨mes-dencodage-catÃ©goriel)
7. [Violations des Bonnes Pratiques ML](#7-violations-des-bonnes-pratiques-ml)

---

# 1. ERREUR CRITIQUE : Formule Logistique Non Standard

## 1.1 Code Source IncriminÃ©

**Fichier** : `calcul_pdo.py`, ligne 159

```python
df_main_ilc = df_main_ilc.with_columns(
    (1 - 1 / (1 + ((-1 * pl.col("sum_total_coeffs")).exp()))).alias("PDO_compute")
)
```

## 1.2 Analyse MathÃ©matique DÃ©taillÃ©e

### Formule Standard de la RÃ©gression Logistique

La formule **canonique** et **universellement acceptÃ©e** de la rÃ©gression logistique est :

$$P(Y=1|X) = \sigma(z) = \frac{1}{1 + e^{-z}}$$

oÃ¹ $z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_n X_n$

Cette fonction, appelÃ©e **sigmoÃ¯de** ou **fonction logistique**, possÃ¨de les propriÃ©tÃ©s suivantes :
- $\sigma(z) \in ]0, 1[$ pour tout $z \in \mathbb{R}$
- $\sigma(0) = 0.5$
- $\sigma(z) \to 1$ quand $z \to +\infty$
- $\sigma(z) \to 0$ quand $z \to -\infty$
- $\sigma(-z) = 1 - \sigma(z)$ (propriÃ©tÃ© de symÃ©trie)

### Formule UtilisÃ©e dans le Code PDO

La formule implÃ©mentÃ©e est :

$$P_{PDO} = 1 - \frac{1}{1 + e^{-z}}$$

Simplifions algÃ©briquement :

$$P_{PDO} = 1 - \sigma(z) = \frac{1 + e^{-z} - 1}{1 + e^{-z}} = \frac{e^{-z}}{1 + e^{-z}}$$

En multipliant numÃ©rateur et dÃ©nominateur par $e^z$ :

$$P_{PDO} = \frac{1}{e^z + 1} = \frac{1}{1 + e^z} = \sigma(-z)$$

**Conclusion mathÃ©matique** : La formule du code est $\sigma(-z)$, soit **l'inverse** de la sigmoÃ¯de standard.

## 1.3 DÃ©monstration NumÃ©rique de l'Anomalie

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            COMPARAISON : FORMULE STANDARD vs FORMULE PDO                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Soit z = somme des coefficients (plus z est Ã©levÃ©, plus le risque         â”‚
â”‚  devrait Ãªtre Ã©levÃ© selon la sÃ©mantique des coefficients)                   â”‚
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚    z    â”‚ Formule Standard    â”‚ Formule PDO         â”‚ InterprÃ©tationâ”‚   â”‚
â”‚  â”‚         â”‚ Ïƒ(z) = 1/(1+e^-z)   â”‚ 1-Ïƒ(z) = Ïƒ(-z)      â”‚               â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚   -5    â”‚ 0.0067 (0.67%)      â”‚ 0.9933 (99.33%)     â”‚ âš ï¸ INVERSÃ‰   â”‚   â”‚
â”‚  â”‚   -3    â”‚ 0.0474 (4.74%)      â”‚ 0.9526 (95.26%)     â”‚ âš ï¸ INVERSÃ‰   â”‚   â”‚
â”‚  â”‚   -1    â”‚ 0.2689 (26.89%)     â”‚ 0.7311 (73.11%)     â”‚ âš ï¸ INVERSÃ‰   â”‚   â”‚
â”‚  â”‚    0    â”‚ 0.5000 (50.00%)     â”‚ 0.5000 (50.00%)     â”‚ âœ… Identique  â”‚   â”‚
â”‚  â”‚   +1    â”‚ 0.7311 (73.11%)     â”‚ 0.2689 (26.89%)     â”‚ âš ï¸ INVERSÃ‰   â”‚   â”‚
â”‚  â”‚   +3    â”‚ 0.9526 (95.26%)     â”‚ 0.0474 (4.74%)      â”‚ âš ï¸ INVERSÃ‰   â”‚   â”‚
â”‚  â”‚   +5    â”‚ 0.9933 (99.33%)     â”‚ 0.0067 (0.67%)      â”‚ âš ï¸ INVERSÃ‰   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                             â”‚
â”‚  CONSÃ‰QUENCE CRITIQUE :                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                    â”‚
â”‚  Avec la formule PDO, un coefficient positif (ex: +3.92 pour reboot_score   â”‚
â”‚  classe 1, censÃ© indiquer un risque Ã‰LEVÃ‰) produit une probabilitÃ© BASSE.   â”‚
â”‚                                                                             â”‚
â”‚  Cela signifie soit :                                                       â”‚
â”‚  1. Les coefficients ont Ã©tÃ© calibrÃ©s avec le signe INVERSÃ‰                 â”‚
â”‚  2. La formule est ERRONÃ‰E et le modÃ¨le prÃ©dit l'inverse du risque          â”‚
â”‚  3. Le PDO mesure la "non-dÃ©faillance" et non la "dÃ©faillance"              â”‚
â”‚                                                                             â”‚
â”‚  AUCUNE de ces situations n'est documentÃ©e dans le code.                    â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 1.4 Impact et GravitÃ©

| Aspect | Ã‰valuation |
|--------|------------|
| **GravitÃ©** | ğŸ”´ CRITIQUE |
| **ConformitÃ© acadÃ©mique** | âŒ Non conforme |
| **Documentation** | âŒ Absente |
| **ReproductibilitÃ©** | âŒ Impossible Ã  vÃ©rifier |
| **AuditabilitÃ© rÃ©glementaire** | âŒ Non justifiable |

### Pourquoi c'est Grave

1. **Confusion sÃ©mantique** : Sans documentation, impossible de savoir ce que PDO mesure rÃ©ellement
2. **Non-standard** : Tout data scientist s'attendant Ã  une sigmoÃ¯de standard sera induit en erreur
3. **Risque d'erreur d'interprÃ©tation** : Les mÃ©tiers peuvent mal interprÃ©ter les scores
4. **Non-conformitÃ© rÃ©glementaire** : Un auditeur BÃ¢le/IFRS 9 demandera des justifications

## 1.5 Correction RecommandÃ©e

```python
# OPTION 1 : Utiliser la formule standard (si les coefficients sont corrects)
df_main_ilc = df_main_ilc.with_columns(
    (1 / (1 + (-pl.col("sum_total_coeffs")).exp())).alias("PDO_compute")
)

# OPTION 2 : Si la sÃ©mantique actuelle est correcte, DOCUMENTER explicitement
# et renommer pour Ã©viter toute confusion
df_main_ilc = df_main_ilc.with_columns(
    # ATTENTION: Cette formule calcule P(Non-DÃ©faut) = 1 - sigmoid(z)
    # car les coefficients ont Ã©tÃ© calibrÃ©s pour prÃ©dire le log-odds de survie
    (1 - 1 / (1 + (-pl.col("sum_total_coeffs")).exp())).alias("P_NON_DEFAUT")
)
```

---

# 2. ERREUR CRITIQUE : Intercept Non UtilisÃ©

## 2.1 Code Source IncriminÃ©

**Fichier** : `calcul_pdo.py`, lignes 135-156

```python
# Ligne 136 : L'intercept est dÃ©fini
df_main_ilc = df_main_ilc.with_columns(pl.lit(-3.86402362750751).alias("intercept"))

# Lignes 139-156 : L'intercept N'EST PAS ajoutÃ© Ã  la somme !
df_main_ilc = df_main_ilc.with_columns(
    (
        pl.col("nat_jur_a_coeffs")
        + pl.col("secto_b_coeffs")
        + pl.col("seg_nae_coeffs")
        + pl.col("top_ga_coeffs")
        + pl.col("nbj_coeffs")
        + pl.col("solde_cav_char_coeffs")
        + pl.col("reboot_score_char2_coeffs")
        + pl.col("remb_sepa_max_coeffs")
        + pl.col("pres_prlv_retourne_coeffs")
        + pl.col("pres_saisie_coeffs")
        + pl.col("net_int_turnover_coeffs")
        + pl.col("rn_ca_conso_023b_coeffs")
        + pl.col("caf_dmlt_005_coeffs")
        + pl.col("res_total_passif_035_coeffs")
        + pl.col("immob_total_passif_055_coeffs")
        # âš ï¸ MANQUE : + pl.col("intercept")
    ).alias("sum_total_coeffs")
)
```

## 2.2 Analyse de l'Erreur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INTERCEPT : CODE MORT OU BUG ?                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  L'intercept (Î²â‚€ = -3.864) est CRÃ‰Ã‰ mais JAMAIS UTILISÃ‰ dans le calcul.    â”‚
â”‚                                                                             â”‚
â”‚  FORMULE ATTENDUE :                                                         â”‚
â”‚  z = Î²â‚€ + Î²â‚Xâ‚ + Î²â‚‚Xâ‚‚ + ... + Î²â‚™Xâ‚™                                         â”‚
â”‚  z = -3.864 + nat_jur_coeffs + secto_coeffs + ...                          â”‚
â”‚                                                                             â”‚
â”‚  FORMULE IMPLÃ‰MENTÃ‰E :                                                      â”‚
â”‚  z = nat_jur_coeffs + secto_coeffs + ...                                   â”‚
â”‚  (l'intercept est ignorÃ©)                                                   â”‚
â”‚                                                                             â”‚
â”‚  IMPACT QUANTIFIÃ‰ :                                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚
â”‚                                                                             â”‚
â”‚  Cas baseline (tous coefficients = 0) :                                     â”‚
â”‚                                                                             â”‚
â”‚  â€¢ AVEC intercept (-3.864) :                                                â”‚
â”‚    z = -3.864                                                               â”‚
â”‚    Ïƒ(-z) = Ïƒ(3.864) = 0.979 â†’ PDO = 97.9%                                  â”‚
â”‚    (ou Ïƒ(z) = 0.021 â†’ PDO = 2.1% si formule standard)                      â”‚
â”‚                                                                             â”‚
â”‚  â€¢ SANS intercept (code actuel) :                                           â”‚
â”‚    z = 0                                                                    â”‚
â”‚    Ïƒ(-z) = Ïƒ(0) = 0.5 â†’ PDO = 50%                                          â”‚
â”‚                                                                             â”‚
â”‚  Ã‰CART : |97.9% - 50%| = 47.9 points de pourcentage !                       â”‚
â”‚                                                                             â”‚
â”‚  Cela signifie que TOUS les scores PDO sont potentiellement FAUX.           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.3 HypothÃ¨ses et Investigation

| HypothÃ¨se | ProbabilitÃ© | VÃ©rification |
|-----------|-------------|--------------|
| Bug de code (oubli) | 40% | Comparer avec documentation mÃ©tier |
| Intercept intÃ©grÃ© ailleurs | 30% | VÃ©rifier si les seuils de discrÃ©tisation l'incluent |
| Code mort (copiÃ©-collÃ©) | 20% | VÃ©rifier l'historique Git |
| Choix dÃ©libÃ©rÃ© non documentÃ© | 10% | Rechercher dans les specs |

## 2.4 GravitÃ©

| Aspect | Ã‰valuation |
|--------|------------|
| **Impact sur les scores** | ğŸ”´ Potentiellement tous les scores sont dÃ©calÃ©s |
| **DÃ©tectabilitÃ©** | âš ï¸ Non dÃ©tectable sans validation externe |
| **Type d'erreur** | Bug silencieux (le code s'exÃ©cute sans erreur) |

---

# 3. PROBLÃˆMES DE TYPAGE DES FEATURES

## 3.1 Inventaire du Typage Actuel

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANALYSE DU TYPAGE DES FEATURES                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  FEATURE            â”‚ TYPE UTILISÃ‰  â”‚ TYPE ATTENDU  â”‚ PROBLÃˆME             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  nat_jur_a          â”‚ String        â”‚ Categorical   â”‚ ğŸ”´ Comparaison ==    â”‚
â”‚                     â”‚ ("4-6",">=7") â”‚ ou OneHot     â”‚    sur strings       â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  secto_b            â”‚ String        â”‚ Categorical   â”‚ ğŸ”´ Pas d'ordre       â”‚
â”‚                     â”‚ ("1","2","3") â”‚ ou OneHot     â”‚    sÃ©mantique        â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  solde_cav_char     â”‚ String        â”‚ Ordinal ou    â”‚ ğŸ”´ Ordre perdu       â”‚
â”‚                     â”‚ ("1","2"...)  â”‚ Float         â”‚    (1<2<3<4)         â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  reboot_score_char2 â”‚ String        â”‚ Float         â”‚ ğŸ”´ Score continu     â”‚
â”‚                     â”‚ ("1"..."9")   â”‚ (original)    â”‚    discrÃ©tisÃ©        â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  VB005, VB035, VB055â”‚ Float â†’ Stringâ”‚ Float         â”‚ ğŸ”´ Perte prÃ©cision   â”‚
â”‚                     â”‚ (discrÃ©tisÃ©)  â”‚ (continu)     â”‚                      â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  remb_sepa_max      â”‚ String        â”‚ Boolean       â”‚ ğŸ”´ "1"/"2" vs        â”‚
â”‚                     â”‚ ("1" ou "2")  â”‚ ou Int        â”‚    True/False        â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  pres_prlv_retourne â”‚ String        â”‚ Boolean       â”‚ ğŸ”´ Idem              â”‚
â”‚                     â”‚ ("1" ou "2")  â”‚               â”‚                      â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â”‚  pres_saisie        â”‚ String        â”‚ Boolean       â”‚ ğŸ”´ Idem              â”‚
â”‚                     â”‚ ("1" ou "2")  â”‚               â”‚                      â”‚
â”‚                     â”‚               â”‚               â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.2 ProblÃ¨me 1 : Variables Continues TransformÃ©es en Strings

### Code ProblÃ©matique

**Fichier** : `preprocessing_format_variables.py`

```python
# Le score Reboot est un FLOAT continu entre 0 et 1
# Il est transformÃ© en STRING avec 9 classes !
df_main = df_main.with_columns(
    pl.when(pl.col("reboot_score2") < 0.00142771716)
    .then(pl.lit("1"))  # â† STRING "1" au lieu de catÃ©gorie
    .when((pl.col("reboot_score2") >= 0.00142771716) & (pl.col("reboot_score2") < 0.00274042692))
    .then(pl.lit("2"))  # â† STRING "2"
    # ...
    .alias("reboot_score_char2")
)
```

### Pourquoi c'est ProblÃ©matique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PERTE D'INFORMATION PAR TYPAGE STRING                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. PERTE DE L'ORDINALITÃ‰                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚                                                                             â”‚
â”‚     En Python/Polars, "2" > "1" fonctionne par comparaison lexicographique â”‚
â”‚     MAIS "10" < "2" car "1" < "2" en ASCII !                               â”‚
â”‚                                                                             â”‚
â”‚     Le modÃ¨le ne peut pas exploiter l'ordre naturel des classes.            â”‚
â”‚                                                                             â”‚
â”‚  2. COMPARAISONS FRAGILES                                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                  â”‚
â”‚                                                                             â”‚
â”‚     ```python                                                               â”‚
â”‚     pl.when(pl.col("solde_cav_char") == "2")  # Comparaison de strings     â”‚
â”‚     ```                                                                     â”‚
â”‚                                                                             â”‚
â”‚     Risques :                                                               â”‚
â”‚     â€¢ "2" vs "2 " (espace trailing) â†’ False                                â”‚
â”‚     â€¢ "2" vs "02" â†’ False                                                  â”‚
â”‚     â€¢ Sensible aux encodages (UTF-8, ASCII, etc.)                          â”‚
â”‚                                                                             â”‚
â”‚  3. MÃ‰MOIRE INEFFICACE                                                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚                                                                             â”‚
â”‚     â€¢ String "1" : ~50 bytes (avec overhead Python)                         â”‚
â”‚     â€¢ Int8 1 : 1 byte                                                       â”‚
â”‚     â€¢ Factor/Categorical : ~4 bytes + table de lookup                       â”‚
â”‚                                                                             â”‚
â”‚     Pour 100K observations Ã— 15 features :                                  â”‚
â”‚     â€¢ Strings : ~75 MB                                                      â”‚
â”‚     â€¢ Integers : ~1.5 MB                                                    â”‚
â”‚                                                                             â”‚
â”‚     Facteur 50x de gaspillage mÃ©moire !                                     â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.3 ProblÃ¨me 2 : Variables Binaires EncodÃ©es "1"/"2" au lieu de 0/1

### Code ProblÃ©matique

```python
# preprocessing_transac.py
df_transac = df_transac.with_columns(
    pl.when(pl.col("rembt_prlv_sepa__max_amount") > 3493.57007)
    .then(pl.lit("1"))      # â† Devrait Ãªtre 1 (int) ou True
    .otherwise(pl.lit("2")) # â† Devrait Ãªtre 0 (int) ou False
    .alias("remb_sepa_max")
)
```

### Pourquoi c'est Anti-Pattern

| Pratique Actuelle | Standard Data Science | ProblÃ¨me |
|-------------------|----------------------|----------|
| "1" = condition vraie | 1 ou True | String inutile |
| "2" = condition fausse | 0 ou False | "2" n'a pas de sens sÃ©mantique |
| Coefficient pour "2" | Coefficient pour 1 | Inversion de la logique |

### Impact sur les Coefficients

Dans le fichier `calcul_pdo.py` :

```python
# Le coefficient est appliquÃ© quand remb_sepa_max == "2"
# Mais "2" signifie "montant <= seuil" (pas de remboursement Ã©levÃ©)
pl.when(pl.col("remb_sepa_max") == "2")
.then(pl.lit(1.34614367878806))  # Coefficient POSITIF pour "pas de remb Ã©levÃ©" ?!
.otherwise(0)
```

**Question critique** : Le coefficient positif est-il appliquÃ© au bon groupe ?

## 3.4 ProblÃ¨me 3 : Absence de Type Polars `Categorical`

### Ã‰tat de l'Art : Polars Categorical

```python
# BONNE PRATIQUE : Utiliser le type Categorical de Polars
df = df.with_columns(
    pl.col("secto_b").cast(pl.Categorical).alias("secto_b_cat")
)

# Avantages :
# 1. Stockage optimisÃ© (table de lookup)
# 2. Comparaisons rapides (entiers en interne)
# 3. Validation automatique des valeurs
# 4. Support natif des opÃ©rations catÃ©gorielles
```

### Code Actuel : Strings Partout

```python
# Le code compare des STRINGS Ã  chaque prÃ©diction
pl.when(pl.col("nat_jur_a") == "4-6")  # Comparaison O(n) sur strings
```

---

# 4. DÃ‰FAUTS DE FEATURE ENGINEERING

## 4.1 DiscrÃ©tisation avec Seuils "Magiques"

### Code IncriminÃ©

```python
# preprocessing_format_variables.py
pl.when(pl.col("solde_cav") < -9.10499954)      # D'oÃ¹ vient -9.105 ?
.then(pl.lit("1"))
.when(pl.col("solde_cav") < 15235.6445)         # D'oÃ¹ vient 15235.64 ?
.then(pl.lit("2"))
.when(pl.col("solde_cav") < 76378.7031)         # D'oÃ¹ vient 76378.70 ?
.then(pl.lit("3"))
```

### Analyse Critique

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEUILS DE DISCRÃ‰TISATION : AUDIT                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  VARIABLE          â”‚ SEUILS                        â”‚ ORIGINE DOCUMENTÃ‰E ?  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  solde_cav         â”‚ -9.105, 15235.64, 76378.70   â”‚ âŒ NON                â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  reboot_score2     â”‚ 0.00143, 0.00274, 0.00564,   â”‚ âŒ NON                â”‚
â”‚                    â”‚ 0.01027, 0.01290, 0.01471,   â”‚                       â”‚
â”‚                    â”‚ 0.01600, 0.04563             â”‚                       â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  VB023             â”‚ 0.431, 2.998                  â”‚ âŒ NON                â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  VB005             â”‚ 66.22                         â”‚ âŒ NON                â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  VB035             â”‚ -8.194, 2.020, 7.104         â”‚ âŒ NON                â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  VB055             â”‚ 22.643, 47.462               â”‚ âŒ NON                â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  Q_JJ_DEPST_MM     â”‚ 12                            â”‚ âŒ NON                â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  rembt_prlv_sepa   â”‚ 3493.57                       â”‚ âŒ NON                â”‚
â”‚    __max_amount    â”‚                               â”‚                       â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  net_interets_sur  â”‚ -0.00144                      â”‚ âŒ NON                â”‚
â”‚    _turnover       â”‚                               â”‚                       â”‚
â”‚                    â”‚                               â”‚                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                             â”‚
â”‚  PROBLÃˆMES IDENTIFIÃ‰S :                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                     â”‚
â”‚                                                                             â”‚
â”‚  1. PRÃ‰CISION EXCESSIVE                                                     â”‚
â”‚     -9.10499954 suggÃ¨re une calibration sur donnÃ©es spÃ©cifiques.            â”‚
â”‚     Risque de sur-ajustement aux donnÃ©es historiques.                       â”‚
â”‚                                                                             â”‚
â”‚  2. ABSENCE DE JUSTIFICATION                                                â”‚
â”‚     Sont-ce des quantiles ? Des seuils mÃ©tier ? Des optimisations ?        â”‚
â”‚     Impossible de valider ou challenger sans documentation.                 â”‚
â”‚                                                                             â”‚
â”‚  3. NON-ADAPTATIFS                                                          â”‚
â”‚     Les seuils sont figÃ©s. Si la distribution des soldes change            â”‚
â”‚     (inflation, changement de clientÃ¨le), les seuils deviennent            â”‚
â”‚     inadaptÃ©s.                                                              â”‚
â”‚                                                                             â”‚
â”‚  4. ASYMÃ‰TRIE NON JUSTIFIÃ‰E                                                 â”‚
â”‚     Pourquoi 4 classes pour solde_cav mais 2 pour VB005 ?                  â”‚
â”‚     Pourquoi 9 classes pour reboot_score2 ?                                 â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4.2 Perte d'Information par DiscrÃ©tisation

### Quantification de la Perte

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PERTE D'INFORMATION : THÃ‰ORIE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  Selon la thÃ©orie de l'information (Shannon), la discrÃ©tisation             â”‚
â”‚  d'une variable continue entraÃ®ne une perte d'entropie.                     â”‚
â”‚                                                                             â”‚
â”‚  FORMULE (approximation) :                                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚                                                                             â”‚
â”‚  Information_retenue â‰ˆ log2(k) / H(X)                                       â”‚
â”‚                                                                             â”‚
â”‚  oÃ¹ k = nombre de classes et H(X) = entropie de X                          â”‚
â”‚                                                                             â”‚
â”‚  APPLICATION AU PDO :                                                       â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚                                                                             â”‚
â”‚  Variable        â”‚ Classes â”‚ Info retenue â”‚ Info perdue                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  solde_cav       â”‚ 4       â”‚ ~35%         â”‚ ~65%                            â”‚
â”‚  reboot_score2   â”‚ 9       â”‚ ~50%         â”‚ ~50%                            â”‚
â”‚  VB023           â”‚ 3       â”‚ ~30%         â”‚ ~70%                            â”‚
â”‚  VB005           â”‚ 2       â”‚ ~20%         â”‚ ~80%                            â”‚
â”‚  VB035           â”‚ 4       â”‚ ~35%         â”‚ ~65%                            â”‚
â”‚  VB055           â”‚ 3       â”‚ ~30%         â”‚ ~70%                            â”‚
â”‚                                                                             â”‚
â”‚  PERTE MOYENNE ESTIMÃ‰E : 60-65% de l'information originale                  â”‚
â”‚                                                                             â”‚
â”‚  âš ï¸ Ces estimations supposent une distribution uniforme des classes.       â”‚
â”‚     Avec des classes dÃ©sÃ©quilibrÃ©es, la perte peut Ãªtre encore plus grande.â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Effet de Bord aux Seuils

```python
# Exemple : deux entreprises quasi-identiques
entreprise_A = {"solde_cav": 15235.64}  # â†’ Classe "2"
entreprise_B = {"solde_cav": 15235.65}  # â†’ Classe "3"

# DiffÃ©rence de solde : 0.01â‚¬
# DiffÃ©rence de coefficient : 0.476 - 0.138 = 0.338
# Impact sur le PDO : significatif !
```

## 4.3 Features d'Interaction : Totalement Absentes

### Ã‰tat de l'Art

Les modÃ¨les modernes capturent automatiquement les interactions (arbres, neural nets), mais mÃªme pour une rÃ©gression logistique, les interactions explicites sont essentielles.

### Exemples d'Interactions Manquantes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INTERACTIONS NON MODÃ‰LISÃ‰ES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  INTERACTION                          â”‚ SIGNIFICATION MÃ‰TIER                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                       â”‚                                     â”‚
â”‚  Taille Ã— Secteur                     â”‚ Une PME dans le BTP n'a pas le      â”‚
â”‚  (nat_jur_a Ã— secto_b)                â”‚ mÃªme profil qu'une PME dans les     â”‚
â”‚                                       â”‚ services                            â”‚
â”‚                                       â”‚                                     â”‚
â”‚  Solde Ã— Score Reboot                 â”‚ Un solde faible + score dÃ©gradÃ©     â”‚
â”‚  (solde_cav Ã— reboot_score)           â”‚ est plus risquÃ© que la somme        â”‚
â”‚                                       â”‚ des deux effets                     â”‚
â”‚                                       â”‚                                     â”‚
â”‚  RentabilitÃ© Ã— Endettement            â”‚ Une faible rentabilitÃ© est plus     â”‚
â”‚  (VB023 Ã— VB005)                       â”‚ grave si l'endettement est Ã©levÃ©   â”‚
â”‚                                       â”‚                                     â”‚
â”‚  Incidents Ã— TrÃ©sorerie               â”‚ Des rejets SEPA avec trÃ©sorerie     â”‚
â”‚  (pres_prlv_retourne Ã— solde_cav)     â”‚ nÃ©gative = signal fort              â”‚
â”‚                                       â”‚                                     â”‚
â”‚  Groupe Ã— Bilan                       â”‚ L'appartenance Ã  un groupe          â”‚
â”‚  (top_ga Ã— ratios SAFIR)              â”‚ modifie l'interprÃ©tation des        â”‚
â”‚                                       â”‚ ratios individuels                  â”‚
â”‚                                       â”‚                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                             â”‚
â”‚  IMPACT ESTIMÃ‰ : +5-10% de pouvoir prÃ©dictif                                â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4.4 Features Temporelles : InexploitÃ©es

### DonnÃ©es Disponibles mais Non UtilisÃ©es

Le pipeline charge les **2 derniers bilans** (N et N-1) mais n'utilise que N :

```python
# preprocessing_safir_soc.py, ligne 52
df_soc = df_soc.filter(pl.col("N_bilan_soc").is_in([1, 2]))  # 2 bilans chargÃ©s

# Ligne 196 : Seul le dernier est conservÃ©
df_soc = df_soc.unique(subset=["i_siren"], keep="first")     # N-1 jetÃ© !
```

### Features Temporelles Manquantes

```python
# CE QUI DEVRAIT ÃŠTRE FAIT :
features_evolution = {
    # Ã‰volution des ratios
    "delta_VB005": (VB005_N - VB005_N1) / abs(VB005_N1),
    "delta_VB035": (VB035_N - VB035_N1) / abs(VB035_N1),
    "delta_VB055": (VB055_N - VB055_N1) / abs(VB055_N1),
    
    # Tendance (amÃ©lioration vs dÃ©gradation)
    "trend_rentabilite": sign(VB035_N - VB035_N1),
    
    # VolatilitÃ© inter-exercice
    "volatilite_CAF": std([CAF_N, CAF_N1]),
    
    # Croissance
    "croissance_CA": (CA_N - CA_N1) / CA_N1,
}
```

---

# 5. ABSENCE DE STANDARDS DATA SCIENCE

## 5.1 Checklist des Pratiques Manquantes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STANDARDS DATA SCIENCE : CONFORMITÃ‰                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  PRATIQUE                              â”‚ PRÃ‰SENT â”‚ CONFORMITÃ‰               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  PRÃ‰PARATION DES DONNÃ‰ES               â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚         â”‚                          â”‚
â”‚  â€¢ Analyse exploratoire (EDA)          â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ DÃ©tection des outliers              â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Traitement des valeurs aberrantes   â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Analyse des distributions           â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ VÃ©rification de la qualitÃ©          â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  FEATURE ENGINEERING                   â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚         â”‚                          â”‚
â”‚  â€¢ Normalisation/Standardisation       â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Encodage one-hot des catÃ©gorielles  â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Feature scaling                     â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Polynomial features                 â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Feature crosses/interactions        â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  SÃ‰LECTION DES FEATURES                â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚         â”‚                          â”‚
â”‚  â€¢ Analyse de corrÃ©lation              â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ VIF (multicolinÃ©aritÃ©)              â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Feature importance                  â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Recursive Feature Elimination       â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Information mutuelle                â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  VALIDATION                            â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                    â”‚         â”‚                          â”‚
â”‚  â€¢ Train/Test/Validation split         â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Cross-validation                    â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Stratified sampling                 â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Time-based validation               â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  MÃ‰TRIQUES                             â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                            â”‚         â”‚                          â”‚
â”‚  â€¢ AUC-ROC                             â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Gini coefficient                    â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ KS statistic                        â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Precision/Recall/F1                 â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Calibration curves                  â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Lift charts                         â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  INTERPRÃ‰TABILITÃ‰                      â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚         â”‚                          â”‚
â”‚  â€¢ SHAP values                         â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Partial Dependence Plots            â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ LIME                                â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚  â€¢ Feature contribution analysis       â”‚   âŒ    â”‚ Non conforme             â”‚
â”‚                                        â”‚         â”‚                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                                                                             â”‚
â”‚  SCORE DE CONFORMITÃ‰ : 0/24 (0%)                                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 5.2 Absence de Traitement des Outliers

### ProblÃ¨me

Les variables continues comme `solde_cav`, `VB005`, `VB035` peuvent contenir des valeurs extrÃªmes qui :
1. Faussent les seuils de discrÃ©tisation
2. CrÃ©ent des effets de levier non dÃ©sirÃ©s
3. Ne sont pas reprÃ©sentatives de la population

### Exemple Concret

```python
# Les seuils de solde_cav :
# -9.10499954 | 15235.6445 | 76378.7031

# Cas problÃ©matique :
# Si solde_cav = 10,000,000â‚¬ (outlier), â†’ classe "4" (comme 76,379â‚¬)
# L'information que c'est un cas exceptionnel est perdue
```

### Solution Standard

```python
from scipy.stats import zscore
from sklearn.preprocessing import RobustScaler

# Option 1 : Winsorization (capping)
df["solde_cav_capped"] = df["solde_cav"].clip(
    lower=df["solde_cav"].quantile(0.01),
    upper=df["solde_cav"].quantile(0.99)
)

# Option 2 : RobustScaler (mÃ©diane + IQR)
scaler = RobustScaler()
df["solde_cav_scaled"] = scaler.fit_transform(df[["solde_cav"]])

# Option 3 : Log-transformation pour distributions asymÃ©triques
df["solde_cav_log"] = np.sign(df["solde_cav"]) * np.log1p(np.abs(df["solde_cav"]))
```

## 5.3 Absence de Gestion du DÃ©sÃ©quilibre de Classes

### Contexte

Le dÃ©faut d'entreprise est un Ã©vÃ©nement **rare** (typiquement 1-5% de la population). Sans donnÃ©es labellisÃ©es visibles dans le code, impossible de vÃ©rifier, mais le traitement standard est absent.

### Techniques Standards Manquantes

```python
# 1. SMOTE (Synthetic Minority Over-sampling Technique)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# 2. Class weights dans le modÃ¨le
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(class_weight='balanced')

# 3. Threshold moving
# Au lieu de seuil 0.5, utiliser le seuil optimal de la courbe ROC
```

---

# 6. PROBLÃˆMES D'ENCODAGE CATÃ‰GORIEL

## 6.1 L'Encodage Actuel : Ni One-Hot, Ni Target, Ni Ordinal

### Analyse du Code

```python
# preprocessing_df_main.py : Encodage "manuel" par regroupement
df_main = df_main.with_columns(
    pl.when(pl.col("c_njur_prsne").is_in(["26", "27", "33", "30"]))
    .then(pl.lit("1-3"))      # Groupe 1 : codes 26, 27, 33, 30
    .when(pl.col("c_njur_prsne").is_in(["20", "21", "29", "55", "59", "64"]))
    .then(pl.lit("4-6"))      # Groupe 2 : codes 20, 21, etc.
    .when(pl.col("c_njur_prsne").is_in(["22", "25", "56", "57", "58"]))
    .then(pl.lit("7"))        # Groupe 3
    .otherwise(pl.lit("7"))   # DÃ©faut = Groupe 3
    .alias("c_njur_prsne_enc")
)
```

### ProblÃ¨mes IdentifiÃ©s

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ANALYSE DE L'ENCODAGE CATÃ‰GORIEL                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  PROBLÃˆME 1 : LOGIQUE DE REGROUPEMENT NON DOCUMENTÃ‰E                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Pourquoi les codes 26, 27, 33, 30 sont-ils regroupÃ©s ensemble ?          â”‚
â”‚  â€¢ Pourquoi le groupe s'appelle "1-3" alors qu'il contient 4 codes ?        â”‚
â”‚  â€¢ Quelle est la justification mÃ©tier ou statistique ?                      â”‚
â”‚                                                                             â”‚
â”‚  PROBLÃˆME 2 : NOMMAGE INCOHÃ‰RENT                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚                                                                             â”‚
â”‚  â€¢ "1-3" suggÃ¨re un range numÃ©rique (codes 1 Ã  3)                           â”‚
â”‚  â€¢ Mais les codes rÃ©els sont 26, 27, 33, 30                                 â”‚
â”‚  â€¢ Confusion garantie pour tout nouveau dÃ©veloppeur                         â”‚
â”‚                                                                             â”‚
â”‚  PROBLÃˆME 3 : VALEUR PAR DÃ‰FAUT ARBITRAIRE                                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                  â”‚
â”‚                                                                             â”‚
â”‚  â€¢ Les codes non listÃ©s sont mis dans le groupe "7"                         â”‚
â”‚  â€¢ Pourquoi "7" et pas une catÃ©gorie "AUTRE" explicite ?                    â”‚
â”‚  â€¢ Risque : un nouveau code non prÃ©vu sera mal classÃ© silencieusement      â”‚
â”‚                                                                             â”‚
â”‚  PROBLÃˆME 4 : 200+ CODES SECTORIELS HARDCODÃ‰S                               â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                               â”‚
â”‚                                                                             â”‚
â”‚  Le fichier preprocessing_df_main.py contient plus de 200 codes             â”‚
â”‚  sectoriels hardcodÃ©s dans des listes. C'est :                              â”‚
â”‚  â€¢ Impossible Ã  maintenir                                                   â”‚
â”‚  â€¢ Source d'erreurs (doublons, oublis)                                      â”‚
â”‚  â€¢ Non Ã©volutif (ajout d'un code = modification du code source)            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 6.2 Comparaison avec les Standards Modernes

### Encodage One-Hot (Standard pour catÃ©gorielles nominales)

```python
# Standard scikit-learn
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded = encoder.fit_transform(df[['c_njur_prsne']])

# RÃ©sultat : une colonne binaire par modalitÃ©
# Avantage : pas d'hypothÃ¨se sur l'ordre des catÃ©gories
# InconvÃ©nient : haute dimensionalitÃ© si nombreuses modalitÃ©s
```

### Target Encoding (Standard pour catÃ©gorielles Ã  haute cardinalitÃ©)

```python
# Pour les variables Ã  nombreuses modalitÃ©s (ex: 200 codes sectoriels)
from category_encoders import TargetEncoder

encoder = TargetEncoder()
df['c_sectrl_1_encoded'] = encoder.fit_transform(df['c_sectrl_1'], df['target'])

# RÃ©sultat : chaque modalitÃ© â†’ moyenne de la cible pour cette modalitÃ©
# Avantage : rÃ©duit la dimensionalitÃ©
# Risque : data leakage si mal implÃ©mentÃ© (cross-validation nÃ©cessaire)
```

### Encodage Ordinal (Pour variables ordinales)

```python
# Si l'ordre a un sens (ex: reboot_score classes 1 < 2 < 3...)
from sklearn.preprocessing import OrdinalEncoder

encoder = OrdinalEncoder(categories=[['1', '2', '3', '4']])
df['reboot_score_ord'] = encoder.fit_transform(df[['reboot_score_char2']])

# RÃ©sultat : 0, 1, 2, 3 (entiers prÃ©servant l'ordre)
```

---

# 7. VIOLATIONS DES BONNES PRATIQUES ML

## 7.1 Absence de Pipeline Reproductible

### ProblÃ¨me

Le preprocessing est implÃ©mentÃ© par **transformations successives in-place**, sans possibilitÃ© de :
1. RÃ©appliquer exactement les mÃªmes transformations sur de nouvelles donnÃ©es
2. Versionner les paramÃ¨tres de transformation (seuils, encodings)
3. SÃ©rialiser le pipeline pour dÃ©ploiement

### Solution Standard : Scikit-learn Pipeline

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

# Pipeline reproductible et sÃ©rialisable
preprocessor = ColumnTransformer([
    ('num', StandardScaler(), numerical_features),
    ('cat', OneHotEncoder(), categorical_features),
])

pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', LogisticRegression())
])

# SÃ©rialisation
import joblib
joblib.dump(pipeline, 'pdo_pipeline.pkl')

# Chargement et application sur nouvelles donnÃ©es
pipeline_loaded = joblib.load('pdo_pipeline.pkl')
predictions = pipeline_loaded.predict(new_data)
```

## 7.2 Data Leakage Potentiel

### Risque IdentifiÃ©

Les seuils de discrÃ©tisation semblent avoir Ã©tÃ© calibrÃ©s sur l'ensemble des donnÃ©es. Si ces seuils proviennent de quantiles calculÃ©s sur le jeu de test, il y a **data leakage**.

```python
# MAUVAISE PRATIQUE (leakage) :
seuils = df['solde_cav'].quantile([0.25, 0.5, 0.75])  # CalculÃ© sur TOUT le dataset
df['solde_cav_bin'] = pd.cut(df['solde_cav'], bins=seuils)

# BONNE PRATIQUE :
# 1. Calculer les seuils UNIQUEMENT sur le train
seuils_train = df_train['solde_cav'].quantile([0.25, 0.5, 0.75])

# 2. Appliquer ces seuils fixes au test
df_test['solde_cav_bin'] = pd.cut(df_test['solde_cav'], bins=seuils_train)
```

## 7.3 Absence de Validation CroisÃ©e

### Impact

Sans cross-validation, il est impossible de :
1. Estimer la variance du modÃ¨le
2. DÃ©tecter le sur-ajustement
3. Optimiser les hyperparamÃ¨tres de maniÃ¨re robuste

### ImplÃ©mentation RecommandÃ©e

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

# Cross-validation stratifiÃ©e (prÃ©serve le ratio de classes)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
print(f"AUC: {scores.mean():.3f} (+/- {scores.std()*2:.3f})")
```

## 7.4 Absence de Calibration des ProbabilitÃ©s

### ProblÃ¨me

Une rÃ©gression logistique produit des scores, pas nÃ©cessairement des probabilitÃ©s calibrÃ©es. Un score de 0.3 ne signifie pas forcÃ©ment 30% de chance de dÃ©faut.

### VÃ©rification de la Calibration

```python
from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt

# Courbe de calibration
fraction_positives, mean_predicted = calibration_curve(y_true, y_pred, n_bins=10)

plt.plot(mean_predicted, fraction_positives, 's-', label='Model')
plt.plot([0, 1], [0, 1], '--', label='Parfaitement calibrÃ©')
plt.xlabel('ProbabilitÃ© prÃ©dite')
plt.ylabel('Fraction rÃ©elle de positifs')
plt.legend()
```

### Correction de la Calibration

```python
from sklearn.calibration import CalibratedClassifierCV

# Calibration par isotonic regression ou Platt scaling
calibrated_model = CalibratedClassifierCV(
    model, 
    method='isotonic',  # ou 'sigmoid'
    cv=5
)
calibrated_model.fit(X_train, y_train)
```

---

# 8. SYNTHÃˆSE ET RECOMMANDATIONS

## 8.1 Matrice de GravitÃ© des ProblÃ¨mes

| # | ProblÃ¨me | GravitÃ© | Impact Business | ComplexitÃ© Fix |
|---|----------|---------|-----------------|----------------|
| 1 | Formule logistique inversÃ©e | ğŸ”´ CRITIQUE | Scores potentiellement inversÃ©s | Moyenne |
| 2 | Intercept non utilisÃ© | ğŸ”´ CRITIQUE | Biais systÃ©matique sur tous les scores | Faible |
| 3 | Typage string au lieu de categorical | ğŸŸ  Ã‰LEVÃ‰ | Performance, maintenabilitÃ© | Moyenne |
| 4 | Seuils de discrÃ©tisation non documentÃ©s | ğŸŸ  Ã‰LEVÃ‰ | Non-auditabilitÃ©, non-reproductibilitÃ© | Ã‰levÃ©e |
| 5 | Absence de features d'interaction | ğŸŸ¡ MOYEN | Perte de pouvoir prÃ©dictif | Moyenne |
| 6 | Absence de features temporelles | ğŸŸ¡ MOYEN | Tendances non capturÃ©es | Moyenne |
| 7 | Encodage catÃ©goriel non standard | ğŸŸ¡ MOYEN | SuboptimalitÃ© | Moyenne |
| 8 | Absence de validation | ğŸ”´ CRITIQUE | Aucune garantie de performance | Ã‰levÃ©e |

## 8.2 Actions ImmÃ©diates Requises

### PrioritÃ© 1 : Corrections Critiques (1 semaine)

```python
# 1. VÃ©rifier et corriger la formule logistique
# AVANT :
(1 - 1 / (1 + ((-1 * pl.col("sum_total_coeffs")).exp())))

# APRÃˆS (si les coefficients sont corrects) :
(1 / (1 + (-pl.col("sum_total_coeffs")).exp()))

# 2. Inclure l'intercept
sum_total_coeffs = (
    pl.col("intercept")  # AJOUTER
    + pl.col("nat_jur_a_coeffs")
    + pl.col("secto_b_coeffs")
    # ...
)

# 3. Documenter explicitement la sÃ©mantique du score
"""
PDO : ProbabilitÃ© de DÃ©faut ObservÃ©e Ã  12 mois
- 0.0 = Risque nul
- 1.0 = DÃ©faut certain
- Formule : Ïƒ(Î²â‚€ + Î£Î²áµ¢xáµ¢) oÃ¹ Ïƒ est la fonction sigmoÃ¯de standard
"""
```

### PrioritÃ© 2 : Refactoring du Typage (2 semaines)

```python
# Remplacer tous les strings par des types appropriÃ©s

# Variables binaires : Boolean ou Int8
df = df.with_columns([
    (pl.col("remb_sepa_max_str") == "2").cast(pl.Boolean).alias("has_high_sepa_refund"),
    (pl.col("pres_saisie_str") == "2").cast(pl.Boolean).alias("has_seizure"),
])

# Variables catÃ©gorielles : Categorical
df = df.with_columns([
    pl.col("nat_jur_a").cast(pl.Categorical).alias("nat_jur_a_cat"),
    pl.col("secto_b").cast(pl.Categorical).alias("secto_b_cat"),
])

# Variables ordinales : Int8 avec ordre prÃ©servÃ©
df = df.with_columns([
    pl.col("solde_cav_char").cast(pl.Int8).alias("solde_cav_class"),
])
```

### PrioritÃ© 3 : Documentation et Validation (3 semaines)

1. **Documenter tous les seuils** avec leur origine (quantiles, mÃ©tier, optimisation)
2. **CrÃ©er un jeu de test** avec des labels de dÃ©faut
3. **Calculer les mÃ©triques** (AUC, Gini, KS) sur ce jeu de test
4. **VÃ©rifier la calibration** avec une courbe de calibration

## 8.3 Architecture Cible

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ARCHITECTURE DATA SCIENCE CIBLE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚           DONNÃ‰ES BRUTES            â”‚
                    â”‚  (Starburst / Data Lake)            â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FEATURE STORE                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Features versionnÃ©es avec mÃ©tadonnÃ©es :                            â”‚  â”‚
â”‚  â”‚  â€¢ Type (numerical, categorical, ordinal, binary)                   â”‚  â”‚
â”‚  â”‚  â€¢ Distribution de rÃ©fÃ©rence                                        â”‚  â”‚
â”‚  â”‚  â€¢ Seuils de discrÃ©tisation (si applicable) + justification         â”‚  â”‚
â”‚  â”‚  â€¢ Date de derniÃ¨re mise Ã  jour                                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PREPROCESSING PIPELINE                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Validation des donnÃ©es (schÃ©ma, types, ranges)                  â”‚  â”‚
â”‚  â”‚  2. Traitement des valeurs manquantes (stratÃ©gie documentÃ©e)        â”‚  â”‚
â”‚  â”‚  3. DÃ©tection et traitement des outliers                            â”‚  â”‚
â”‚  â”‚  4. Normalisation/Scaling (si nÃ©cessaire)                           â”‚  â”‚
â”‚  â”‚  5. Encodage catÃ©goriel (one-hot, target, ordinal selon le cas)     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE ENGINEERING                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Features de base (actuelles)                                    â”‚  â”‚
â”‚  â”‚  2. Features d'interaction (tailleÃ—secteur, soldeÃ—incidents, etc.)  â”‚  â”‚
â”‚  â”‚  3. Features temporelles (Ã©volutions N vs N-1, tendances)           â”‚  â”‚
â”‚  â”‚  4. Features agrÃ©gÃ©es (statistiques sur les transactions)           â”‚  â”‚
â”‚  â”‚  5. Indicateurs de missing (is_null features)                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FEATURE SELECTION                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  1. Ã‰limination variance nulle                                      â”‚  â”‚
â”‚  â”‚  2. Analyse de corrÃ©lation (Ã©liminer r > 0.95)                      â”‚  â”‚
â”‚  â”‚  3. VIF pour multicolinÃ©aritÃ©                                       â”‚  â”‚
â”‚  â”‚  4. Feature importance (SHAP, permutation)                          â”‚  â”‚
â”‚  â”‚  5. SÃ©lection finale (top-k features)                               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MODÃˆLE                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  â€¢ Formule standard : P = Ïƒ(Î²â‚€ + Î£Î²áµ¢xáµ¢)                             â”‚  â”‚
â”‚  â”‚  â€¢ EntraÃ®nement avec cross-validation                               â”‚  â”‚
â”‚  â”‚  â€¢ Calibration des probabilitÃ©s (Platt/Isotonic)                    â”‚  â”‚
â”‚  â”‚  â€¢ MÃ©triques : AUC, Gini, KS, Brier score                           â”‚  â”‚
â”‚  â”‚  â€¢ ExplicabilitÃ© : SHAP values pour chaque prÃ©diction               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         SCORE PDO CALIBRÃ‰           â”‚
                    â”‚  â€¢ ProbabilitÃ© [0, 1]               â”‚
                    â”‚  â€¢ Intervalle de confiance          â”‚
                    â”‚  â€¢ Top 3 features contributives     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Fin du rapport d'audit Data Science**

*Ce rapport identifie des dÃ©fauts critiques qui nÃ©cessitent une attention immÃ©diate avant toute mise en production ou utilisation des scores PDO pour des dÃ©cisions mÃ©tier.*
