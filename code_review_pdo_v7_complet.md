# CODE REVIEW EXHAUSTIF - Projet PDO (ap01202-record-pdo)
## VERSION 7 - RAPPORT COMPLET FUSIONN√â

**Date :** 27 janvier 2026  
**Reviewer :** Tech Lead IA  
**Fichiers analys√©s :** 103  
**Lignes de code :** ~13 290 (Python)

---

# ‚õî VERDICT : NON APPROUV√â POUR PRODUCTION

| Cat√©gorie | Nombre |
|-----------|--------|
| üî¥ Issues Critiques | 8 |
| üü† Issues Haute Priorit√© | 11 |
| üü° Issues Moyenne Priorit√© | 15 |
| üü¢ Issues Basse Priorit√© | 2 |
| **Total** | **36** |

---

# üî¥ ISSUES CRITIQUES

---

## CRITIQUE-001 : Mot de passe Artifactory en clair

| Attribut | Valeur |
|----------|--------|
| **Script** | `config/services/services_dev.env`, `services_pprod.env`, `services_prod.env` |
| **Ligne** | 14 (dev), 12 (pprod), 12 (prod) |

**Code probl√©matique :**
```properties
ARTIFACTORY_PASSWORD=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

**Ce qu'il y a √† corriger :**  
Le mot de passe Artifactory est stock√© en clair dans les fichiers de configuration. De plus, c'est le **m√™me mot de passe** pour les 3 environnements (dev, pprod, prod), ce qui viole le principe de s√©paration des environnements.

**Impact potentiel :**  
- Compromission de l'infrastructure Artifactory si le repo est expos√©
- Acc√®s non autoris√© aux packages priv√©s BNP
- Violation des politiques de s√©curit√© (PCI-DSS, SOX)
- Un attaquant avec acc√®s au repo peut compromettre les 3 environnements

**Solution propos√©e :**  
Stocker les credentials dans un gestionnaire de secrets (Vault, AWS Secrets Manager) et les injecter via variables d'environnement au runtime. R√©voquer imm√©diatement le token actuel et en g√©n√©rer un nouveau par environnement.

```python
# Injection via Vault au d√©marrage
vault = VaultConnector(config_path)
artifactory_password = os.getenv("ARTIFACTORY_PASSWORD")  # Inject√© par Vault
```

---

## CRITIQUE-002 : Fichiers .env NON ignor√©s par Git

| Attribut | Valeur |
|----------|--------|
| **Script** | `_gitignore` |
| **Ligne** | 1-2 |

**Code probl√©matique :**
```gitignore
.env
.env.*
```

**Ce qu'il y a √† corriger :**  
Le pattern `.env.*` ne matche PAS les fichiers `services_dev.env`, `services_pprod.env`, `services_prod.env`. Ces fichiers sont donc track√©s dans Git avec leurs credentials.

**Impact potentiel :**  
- Les credentials sont dans l'historique Git, m√™me apr√®s suppression
- Tout d√©veloppeur avec acc√®s au repo voit les mots de passe
- Impossible de r√©voquer l'acc√®s sans nettoyer tout l'historique Git

**Solution propos√©e :**  
Ajouter les patterns corrects au `.gitignore` et nettoyer l'historique Git avec BFG Repo Cleaner.

```gitignore
services_*.env
config/services/*.env
```

---

## CRITIQUE-003 : Donn√©es RGPD √©crites en clair

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/base_transformation.py` |
| **Lignes** | 85-95, 237-251, 260, 269, 278-293 |

**Code probl√©matique :**
```python
df_dict["unfiltered_df_main"].write_csv(
    f"{LOCAL_PATH}/unfiltered_df_main.csv", separator=",", include_header=True
)
```

**Ce qu'il y a √† corriger :**  
22 √©critures CSV de donn√©es bancaires sensibles (SIREN, soldes, scores de risque, transactions) sont effectu√©es en clair sur le syst√®me de fichiers, dans un chemin hardcod√© `/mnt/data/output`.

**Impact potentiel :**  
- Non-conformit√© RGPD (donn√©es personnelles non chiffr√©es)
- Exposition des donn√©es en cas de compromission du serveur
- Pas de contr√¥le d'acc√®s sur les fichiers
- Risque d'amende jusqu'√† 4% du CA annuel

**Solution propos√©e :**  
Supprimer les √©critures de debug en production, ou les conditionner √† un flag `DEBUG`. Si n√©cessaire, chiffrer les fichiers.

```python
if os.getenv("DEBUG_MODE", "false").lower() == "true":
    df_dict["unfiltered_df_main"].write_csv(f"{LOCAL_PATH}/debug.csv")
```

---

## CRITIQUE-004 : Lazy Loading NON utilis√©

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/base_transformation.py` |
| **Lignes** | 112-215 |

**Code probl√©matique :**
```python
df_main_encoded = df_encoding(df_main)       # Mat√©rialise
df_main_reboot = add_reboot_features(...)    # Mat√©rialise
df_main_transac = add_transac_features(...)  # Mat√©rialise
# ... 8 autres mat√©rialisations
```

**Ce qu'il y a √† corriger :**  
Le pipeline effectue 11 mat√©rialisations successives de DataFrames Polars (mode eager). Chaque √©tape cr√©e une copie compl√®te en m√©moire au lieu d'utiliser le lazy evaluation de Polars.

**Impact potentiel :**  
- Perte de 30-50% de performance sur le pipeline complet
- Consommation m√©moire 3x sup√©rieure √† l'optimal
- Risque d'OOM sur gros volumes

**Solution propos√©e :**  
Refactorer le pipeline pour utiliser des LazyFrames Polars et ne mat√©rialiser qu'une seule fois √† la fin avec `.collect()`.

```python
df_lazy = unfiltered_df_main.lazy()
df_lazy = df_encoding_lazy(df_lazy)
df_lazy = add_reboot_features_lazy(df_lazy, reboot.lazy())
df_final = df_lazy.collect()  # Une seule mat√©rialisation
```

---

## CRITIQUE-005 : SQL Injection via .replace()

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/sql/retrieve_sql_query_transac.py` |
| **Lignes** | 78-79 |

**Code probl√©matique :**
```python
sql_query = sql_query.replace("start_date", start_date)
sql_query = sql_query.replace("end_date", end_date)
```

**Ce qu'il y a √† corriger :**  
Les param√®tres `start_date` et `end_date` sont inject√©s dans la requ√™te SQL via un simple `replace()` sans aucune validation ni √©chappement.

**Impact potentiel :**  
- Ex√©cution de code SQL arbitraire
- Exfiltration de donn√©es (SELECT *)
- Modification/suppression de donn√©es (DROP, DELETE)

**Solution propos√©e :**  
Valider strictement le format des dates avec une regex avant injection.

```python
import re
def validate_date(date_str: str) -> str:
    if not re.match(r"^\d{4}-\d{2}-\d{2}$", date_str):
        raise ValueError(f"Invalid date format: {date_str}")
    return date_str

sql_query = sql_query.replace("start_date", validate_date(start_date))
```

---

## CRITIQUE-006 : Faux lazy_query() trompeur

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/con_starburst.py` |
| **Lignes** | 70-76 |

**Code probl√©matique :**
```python
def lazy_query(self, query: str) -> pl.LazyFrame:
    eager_df = self.starburst_manager.get_engine(query)  # Charge TOUT
    return eager_df.lazy()  # Conversion APR√àS = inutile
```

**Ce qu'il y a √† corriger :**  
La m√©thode `lazy_query()` sugg√®re une √©valuation diff√©r√©e, mais charge d'abord 100% des donn√©es en m√©moire puis convertit en LazyFrame. Le "lazy" est trompeur.

**Impact potentiel :**  
- D√©veloppeurs tromp√©s pensant b√©n√©ficier du lazy loading
- Aucun gain de performance malgr√© le nom
- Dette technique et confusion

**Solution propos√©e :**  
Supprimer cette m√©thode ou la renommer avec un docstring explicite.

```python
def query_as_lazyframe(self, query: str) -> pl.LazyFrame:
    """WARNING: Data is fully loaded first. No true lazy evaluation."""
    eager_df = self.starburst_manager.get_engine(query)
    return eager_df.lazy()
```

---

## CRITIQUE-007 : app.sh r√©f√©rence fichier inexistant

| Attribut | Valeur |
|----------|--------|
| **Script** | `app.sh` |
| **Ligne** | (r√©f√©rence √† stream.py) |

**Code probl√©matique :**
```bash
python stream.py  # Fichier inexistant
```

**Ce qu'il y a √† corriger :**  
Le script de d√©marrage `app.sh` r√©f√©rence un fichier `stream.py` qui n'existe pas dans l'arborescence du projet.

**Impact potentiel :**  
- √âchec du d√©ploiement en production
- Erreur `FileNotFoundError` au runtime
- Pipeline CI/CD cass√©

**Solution propos√©e :**  
Supprimer la r√©f√©rence √† `stream.py` ou cr√©er le fichier si n√©cessaire.

```bash
# Supprimer ou commenter la ligne
# python stream.py  # SUPPRIM√â
```

---

## CRITIQUE-008 : Division par z√©ro non prot√©g√©e (SAFIR CONSO)

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/preprocessing/preprocessing_safir_conso.py` |
| **Lignes** | 37, 38, 60, 82 |

**Code probl√©matique :**
```python
# Ligne 37
.then(pl.col("mt_310_conso") / pl.col("c_duree_excce_conso") * 12)

# Ligne 38
.otherwise((pl.col("mt_24") + pl.col("mt_309")) / pl.col("c_duree_excce_conso") * 12)

# Ligne 60
/ pl.col("c_duree_excce_conso")

# Ligne 82
/ pl.col("c_duree_excce_conso")
```

**Ce qu'il y a √† corriger :**  
La colonne `c_duree_excce_conso` est utilis√©e comme diviseur √† 4 endroits **sans aucune v√©rification** que la valeur n'est pas z√©ro ou null. Si cette dur√©e d'exercice comptable est 0 (exercice de dur√©e nulle) ou null (donn√©e manquante), une division par z√©ro se produit.

**Impact potentiel :**  
- **Crash du batch** si une seule ligne a `c_duree_excce_conso = 0`
- Production de valeurs `inf` (infini) si Polars ne l√®ve pas d'exception
- **Corruption silencieuse des donn√©es** : les ratios financiers deviennent invalides
- Propagation des erreurs jusqu'au calcul PDO final
- **Scores PDO incorrects** pour les entreprises concern√©es

**Solution propos√©e :**  
Ajouter une protection avant les divisions : remplacer les valeurs 0 ou null par une valeur par d√©faut (12 mois standard) ou exclure ces lignes du calcul.

```python
# Ajouter AVANT les calculs (apr√®s ligne 29)
df_conso = df_conso.with_columns(
    pl.when((pl.col("c_duree_excce_conso").is_null()) | (pl.col("c_duree_excce_conso") == 0))
    .then(pl.lit(12))  # Dur√©e standard de 12 mois
    .otherwise(pl.col("c_duree_excce_conso"))
    .alias("c_duree_excce_conso")
)
```

---

# üü† ISSUES HAUTE PRIORIT√â

---

## HAUTE-009 : Window functions Polars s√©par√©es

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/sql/retrieve_sql_query_transac.py` |
| **Lignes** | 149-157 |

**Code probl√©matique :**
```python
df = df.with_columns(pl.col("amount").sum().over(["i_uniq_kpi", "category"]).alias("netamount"))
df = df.with_columns(pl.col("amount").min().over(["i_uniq_kpi", "category"]).alias("min_amount"))
df = df.with_columns(pl.col("amount").max().over(["i_uniq_kpi", "category"]).alias("max_amount"))
```

**Ce qu'il y a √† corriger :**  
5 appels s√©par√©s √† `with_columns()` avec des window functions. Chaque appel d√©clenche un scan complet du DataFrame.

**Impact potentiel :**  
- 5 scans au lieu de 1 = ~400% de travail inutile
- Perte de 20-40% de performance sur cette √©tape

**Solution propos√©e :**  
Regrouper toutes les window functions dans un seul appel avec une liste.

```python
df = df.with_columns([
    pl.col("amount").sum().over(["i_uniq_kpi", "category"]).alias("netamount"),
    pl.col("amount").min().over(["i_uniq_kpi", "category"]).alias("min_amount"),
    pl.col("amount").max().over(["i_uniq_kpi", "category"]).alias("max_amount"),
])
```

---

## HAUTE-010 : 5 scans aggregate_category r√©p√©t√©s

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/preprocessing/preprocessing_transac.py` |
| **Lignes** | 98-102 |

**Code probl√©matique :**
```python
df_interets = aggregate_category(donnees_transac_filtered, "interets__", "interets__")
df_turnover = aggregate_category(donnees_transac_filtered, "turnover__", "turnover__")
df_prlv_retourne = aggregate_category(donnees_transac_filtered, "prlv_sepa_retourne__", ...)
```

**Ce qu'il y a √† corriger :**  
La fonction `aggregate_category()` est appel√©e 5 fois sur le m√™me DataFrame, provoquant 5 scans complets.

**Impact potentiel :**  
- 5 scans complets au lieu de 1
- ~20% de perte de performance sur cette √©tape

**Solution propos√©e :**  
Restructurer pour faire toutes les agr√©gations en une seule passe.

```python
categories = ["interets__", "turnover__", "prlv_sepa_retourne__", ...]
df_aggregated = aggregate_all_categories(donnees_transac_filtered, categories)
```

---

## HAUTE-011 : 280+ codes sectoriels hardcod√©s

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/preprocessing/preprocessing_df_main.py` |
| **Lignes** | 21-301 |

**Code probl√©matique :**
```python
.when(pl.col("c_sectrl_1").is_in([
    "420053", "420051", "420050", "420052", "420040", ...
]))
```

**Ce qu'il y a √† corriger :**  
Plus de 280 codes sectoriels sont hardcod√©s directement dans le code Python sans fichier de r√©f√©rence ni versioning.

**Impact potentiel :**  
- Maintenance impossible
- Pas d'audit trail des changements
- Risque d'erreur lors de modifications

**Solution propos√©e :**  
Externaliser les codes dans un fichier YAML versionn√©.

```yaml
# config/application/sector_codes.yml
sector_encoding:
  class_1:
    - "420053"
    - "420051"
```

---

## HAUTE-012 : Seuils ML hardcod√©s (Magic Values)

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/preprocessing/preprocessing_format_variables.py` |
| **Lignes** | 32-133 |

**Code probl√©matique :**
```python
pl.when(pl.col("reboot_score2") < 0.00142771716)
.then(pl.lit("1"))
```

**Ce qu'il y a √† corriger :**  
15+ seuils num√©riques issus de la calibration ML sont hardcod√©s (0.00142771716, 76378.7031, etc.) sans documentation de leur origine.

**Impact potentiel :**  
- Risque de r√©gression silencieuse si seuils modifi√©s
- Audit ML impossible
- Pas de tra√ßabilit√© des calibrations

**Solution propos√©e :**  
Externaliser dans un fichier de configuration avec m√©tadonn√©es.

```yaml
# config/application/ml_thresholds.yml
reboot_score:
  calibration_date: "2025-06-15"
  thresholds:
    class_1: 0.00142771716
```

---

## HAUTE-013 : Z√©ro tests d'int√©gration

| Attribut | Valeur |
|----------|--------|
| **Script** | `tests/int√©gration/` |
| **Ligne** | N/A (dossier vide) |

**Code probl√©matique :**
```
tests/int√©gration/
‚îú‚îÄ‚îÄ __init__.py  # Vide
‚îî‚îÄ‚îÄ industrialisation/
    ‚îî‚îÄ‚îÄ __init__.py  # Vide
```

**Ce qu'il y a √† corriger :**  
Le dossier de tests d'int√©gration existe mais ne contient aucun test. Le flux complet SQL ‚Üí Preprocessing ‚Üí PDO ‚Üí COS n'est jamais test√©.

**Impact potentiel :**  
- R√©gressions non d√©tect√©es avant production
- Bugs d√©couverts uniquement en production
- Confiance r√©duite dans les d√©ploiements

**Solution propos√©e :**  
Cr√©er des tests d'int√©gration avec donn√©es mock√©es.

```python
# tests/int√©gration/test_pipeline_e2e.py
def test_full_pipeline_with_mock_data():
    mock_data = create_mock_starburst_data()
    result = run_full_pipeline(mock_data)
    assert result["df_main_pdo"]["PDO"].is_not_null().all()
```

---

## HAUTE-014 : Duplication code 99.5%

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `common/config_context.py`, `config/config_models.py` |
| **Lignes** | 1-70 (les deux fichiers) |

**Code probl√©matique :**
```python
# config_context.py
class ConfigContext:
    _instance = None
    _config: dict = {}
    
# config_models.py - IDENTIQUE sauf le nom
class ConfigModels:
    _instance = None
    _config: dict = {}
```

**Ce qu'il y a √† corriger :**  
Les deux fichiers sont identiques √† 99.5% (seul le nom de classe change). Violation flagrante du principe DRY.

**Impact potentiel :**  
- Maintenance double
- Risque de divergence entre les deux classes
- Code inutilement dupliqu√©

**Solution propos√©e :**  
Cr√©er une classe de base abstraite et en h√©riter.

```python
# common/base_config.py
class BaseConfigSingleton:
    _instance = None
    _config: dict[str, Any] = {}

# common/config_context.py
class ConfigContext(BaseConfigSingleton):
    pass
```

---

## HAUTE-015 : Exception g√©n√©rique catch-all

| Attribut | Valeur |
|----------|--------|
| **Script** | `industrialisation/src/batch.py` |
| **Ligne** | 146 |

**Code probl√©matique :**
```python
except Exception as e:
    log_batch_error(e)
    print_final_summary()
    raise
```

**Ce qu'il y a √† corriger :**  
Le catch-all `except Exception` capture toutes les exceptions sans distinction, masquant les erreurs sp√©cifiques.

**Impact potentiel :**  
- Erreurs sp√©cifiques masqu√©es
- Debugging difficile
- Logs peu informatifs

**Solution propos√©e :**  
Capturer les exceptions sp√©cifiques d'abord.

```python
except DatabaseConnectionError as e:
    logger.error("Database error: %s", e)
    raise
except Exception as e:
    logger.exception("Unexpected error")
    raise RuntimeError(f"Batch failed: {e}") from e
```

---

## HAUTE-016 : Code mort (train.py 100% comment√©)

| Attribut | Valeur |
|----------|--------|
| **Script** | `exploration/scripts/train.py` |
| **Lignes** | 1-91 |

**Code probl√©matique :**
```python
# import logging
# import os
# import mlflow
# ... 91 lignes comment√©es
```

**Ce qu'il y a √† corriger :**  
Le fichier contient 91 lignes de code, toutes comment√©es. C'est du code mort.

**Impact potentiel :**  
- Confusion pour les d√©veloppeurs
- Dette technique
- Faux positifs dans les analyses de code

**Solution propos√©e :**  
Supprimer le fichier ou le r√©activer s'il est n√©cessaire.

```bash
git rm exploration/scripts/train.py
```

---

## HAUTE-017 : Division par z√©ro partiellement prot√©g√©e (SAFIR SOC)

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/preprocessing/preprocessing_safir_soc.py` |
| **Lignes** | 85, 89, 127, 129 |

**Code probl√©matique :**
```python
# Ligne 85
/ pl.col("c_duree_excce_soc")

# Ligne 89
.then((pl.col("mt_182") + pl.col("mt_285") + pl.col("mt_295")) / pl.col("c_duree_excce_soc") * 12)

# Ligne 127
.then(pl.col("mt_479") / pl.col("c_duree_excce_soc") * 12)

# Ligne 129
.then((pl.col("mt_480") + pl.col("mt_435") - pl.col("mt_209")) / pl.col("c_duree_excce_soc") * 12)
```

**Ce qu'il y a √† corriger :**  
Une protection existe aux lignes 56-63 qui remplace `c_duree_excce_soc = 0` ou `null` par `duree_excce_imp`. **MAIS** cette protection est incompl√®te car `duree_excce_imp` est calcul√© comme :
```python
(d_fin_excce_soc - leg_d_fin_excce).dt.total_days() / 30.44
```
Si les deux dates sont identiques (bilan de dur√©e 0), `duree_excce_imp = 0`, et la division par z√©ro reste possible.

**Impact potentiel :**  
- Division par z√©ro dans des cas edge (bilans de dur√©e 0)
- Valeurs `inf` ou crash
- Donn√©es financi√®res corrompues

**Solution propos√©e :**  
Ajouter une protection suppl√©mentaire apr√®s le calcul de `duree_excce_imp` pour garantir une valeur minimale.

```python
# Apr√®s ligne 63, ajouter une protection finale
df_soc = df_soc.with_columns(
    pl.when(pl.col("c_duree_excce_soc") <= 0)
    .then(pl.lit(12))  # Valeur par d√©faut si toujours <= 0
    .otherwise(pl.col("c_duree_excce_soc"))
    .alias("c_duree_excce_soc")
)
```

---

## HAUTE-018 : Propagation de NaN dans le mod√®le ML

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/calcul_pdo.py` |
| **Lignes** | 497-499 |

**Code probl√©matique :**
```python
X = df_encoded.select(feature_order).to_numpy()
probas = model.predict_proba(X)[:, 0]
```

**Ce qu'il y a √† corriger :**  
La conversion `.to_numpy()` transforme les valeurs `null` Polars en `NaN` numpy. Ces `NaN` sont ensuite pass√©s au mod√®le sklearn `predict_proba()` qui :
1. Peut retourner `NaN` pour les lignes concern√©es
2. Peut lever une exception selon la version sklearn
3. Peut produire des probabilit√©s aberrantes

Aucune v√©rification n'est faite avant ou apr√®s la pr√©diction.

**Impact potentiel :**  
- Scores PDO = `NaN` pour certaines entreprises
- Crash du mod√®le sklearn
- Donn√©es envoy√©es au COS avec des valeurs manquantes/incorrectes
- **D√©cisions de cr√©dit bas√©es sur des donn√©es corrompues**

**Solution propos√©e :**  
V√©rifier l'absence de NaN avant la pr√©diction et g√©rer les cas probl√©matiques.

```python
X = df_encoded.select(feature_order).to_numpy()

# V√©rifier les NaN
if np.isnan(X).any():
    nan_rows = np.where(np.isnan(X).any(axis=1))[0]
    logger.warning("Found %d rows with NaN values in features", len(nan_rows))
    X = np.nan_to_num(X, nan=0.0)  # Ou strat√©gie m√©tier appropri√©e

probas = model.predict_proba(X)[:, 0]
```

---

## HAUTE-019 : Overflow potentiel avec exp()

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `common/calcul_pdo.py`, `common/preprocessing/preprocessing_reboot.py` |
| **Lignes** | calcul_pdo.py:206, 504 / preprocessing_reboot.py:29 |

**Code probl√©matique :**
```python
# calcul_pdo.py:206
(1 - 1 / (1 + (pl.col("score").exp()))).alias("PDO_compute")

# calcul_pdo.py:504
(1 - 1 / (1 + (pl.col("score").exp()))).alias("PDO_compute")

# preprocessing_reboot.py:29
(1 / (1 + ((-1 * pl.col("q_score")).exp()))).alias("q_score2")
```

**Ce qu'il y a √† corriger :**  
La fonction exponentielle `exp(x)` overflow quand `x > 709` (limite float64). Si le score calcul√© est tr√®s grand (positif ou n√©gatif selon le signe), `exp(score)` produit `inf`, ce qui corrompt le calcul de probabilit√©.

**Impact potentiel :**  
- Valeurs `inf` ou `NaN` dans les scores
- Instabilit√© num√©rique
- Probabilit√©s incorrectes

**Solution propos√©e :**  
Utiliser une impl√©mentation stable de la sigmo√Øde qui √©vite les overflow.

```python
# Impl√©mentation stable de la sigmo√Øde
def stable_sigmoid(x):
    return pl.when(x >= 0)
        .then(1 / (1 + (-x).exp()))
        .otherwise(x.exp() / (1 + x.exp()))

# Utilisation
df_main = df_main.with_columns(
    (1 - stable_sigmoid(pl.col("score"))).alias("PDO_compute")
)
```

---

# üü° ISSUES MOYENNE PRIORIT√â

---

## MOYENNE-020 : Fonctions sans docstring

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `base_transformation.py:41`, `version.py:9,17,21`, `custom_profiler.py:14,16`, `generate_confluence_doc.py:41,57,111`, `logging_utils.py:85` |

**Code probl√©matique :**
```python
def __init__(self, app_config: dict, ...):
    self.app_config = app_config  # Pas de docstring
```

**Ce qu'il y a √† corriger :**  
10 fonctions publiques n'ont pas de docstring.

**Impact potentiel :**  
- Code difficile √† comprendre
- Documentation auto-g√©n√©r√©e incompl√®te

**Solution propos√©e :**  
Ajouter des docstrings Google-style.

```python
def __init__(self, app_config: dict, ...):
    """Initialize the BaseTransformation pipeline.
    
    Args:
        app_config: Application configuration with model coefficients.
    """
```

---

## MOYENNE-021 : Type hints incomplets

| Attribut | Valeur |
|----------|--------|
| **Scripts** | 23 fichiers |
| **Lignes** | 74 erreurs Mypy |

**Code probl√©matique :**
```python
def load_data(self) -> dict:  # dict sans param√®tres
def get_prefix_files(self) -> list:  # list sans param√®tres
```

**Ce qu'il y a √† corriger :**  
74 erreurs Mypy : `dict` et `list` sans param√®tres de type.

**Impact potentiel :**  
- Pas de v√©rification de type statique
- IDE moins efficace

**Solution propos√©e :**  
Compl√©ter les type hints avec les param√®tres g√©n√©riques.

```python
def load_data(self) -> dict[str, pl.DataFrame]:
def get_prefix_files(self) -> list[str]:
```

---

## MOYENNE-022 : Nommage FR/EN incoh√©rent

| Attribut | Valeur |
|----------|--------|
| **Scripts** | Tout le projet |

**Code probl√©matique :**
```python
donnees_transac = extract_starburst_transactions(...)  # FR
df_main = unfiltered_df_main.filter(...)               # EN
```

**Ce qu'il y a √† corriger :**  
M√©lange de fran√ßais et anglais dans les noms de variables et fonctions.

**Impact potentiel :**  
- Code moins lisible
- Recherche dans le code difficile

**Solution propos√©e :**  
Standardiser en anglais.

```python
transaction_data = extract_starburst_transactions(...)
filter_main_df(df)
```

---

## MOYENNE-023 : SELECT DISTINCT excessifs

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `common/sql/queries/*.sql` |
| **Lignes** | 7 occurrences (query_starburst_transac.sql) |

**Code probl√©matique :**
```sql
SELECT DISTINCT column1, column2 FROM table
```

**Ce qu'il y a √† corriger :**  
Utilisation de `SELECT DISTINCT` potentiellement inutile si la cl√© primaire garantit l'unicit√©.

**Impact potentiel :**  
- Performance SQL d√©grad√©e
- Tri inutile c√¥t√© base

**Solution propos√©e :**  
V√©rifier si DISTINCT est vraiment n√©cessaire.

```sql
SELECT column1, column2 FROM table  -- Sans DISTINCT si unicit√© garantie
```

---

## MOYENNE-024 : print() au lieu de logger

| Attribut | Valeur |
|----------|--------|
| **Script** | `config_helper/generate_project_config.py` |
| **Lignes** | 25 occurrences |

**Code probl√©matique :**
```python
print("Bienvenue dans l'assistant de g√©n√©ration...")
```

**Ce qu'il y a √† corriger :**  
25 `print()` au lieu du logger. Messages non structur√©s, non filtrables.

**Impact potentiel :**  
- Pas de logs structur√©s
- Impossible de filtrer par niveau

**Solution propos√©e :**  
Remplacer par le logger.

```python
logger.info("Bienvenue dans l'assistant de g√©n√©ration...")
```

---

## MOYENNE-025 : Fichiers sans tests correspondants

| Attribut | Valeur |
|----------|--------|
| **Scripts** | 8 modules sans tests |

**Ce qu'il y a √† corriger :**  
8 modules n'ont pas de fichier de test : `config_models.py`, `log_model.py`, `get_env_var.py`, `train.py`, `constants.py`, `generate_project_config.py`, `extra_parameters_dto.py`, `generate_confluence_doc.py`.

**Impact potentiel :**  
- Couverture de tests incompl√®te
- R√©gressions non d√©tect√©es

**Solution propos√©e :**  
Cr√©er les fichiers de tests manquants.

```python
# tests/unit/config/test_config_models.py
def test_singleton_pattern():
    instance1 = ConfigModels()
    instance2 = ConfigModels()
    assert instance1 is instance2
```

---

## MOYENNE-026 : Encodage UTF-8 cass√© dans docstrings

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `calcul_pdo.py`, `batch.py`, `preprocessing_transac.py` |
| **Lignes** | 284 occurrences |

**Code probl√©matique :**
```python
"""LOGIQUE DES COEFFICIENTS - MOD√ÉÀÜLE P(NON-D√É‚Ä∞FAUT)"""
```

**Ce qu'il y a √† corriger :**  
284 caract√®res UTF-8 mal encod√©s (`√É¬©` au lieu de `√©`).

**Impact potentiel :**  
- Documentation illisible
- Professionnalisme d√©grad√©

**Solution propos√©e :**  
R√©-encoder en UTF-8 correct.

```python
"""LOGIQUE DES COEFFICIENTS - MOD√àLE P(NON-D√âFAUT)"""
```

---

## MOYENNE-027 : Imports non tri√©s (I001)

| Attribut | Valeur |
|----------|--------|
| **Scripts** | 23 fichiers |

**Ce qu'il y a √† corriger :**  
Imports non tri√©s selon l'ordre standard (stdlib ‚Üí third-party ‚Üí local).

**Impact potentiel :**  
- Non-conformit√© PEP8
- CI/CD qui √©choue si isort activ√©

**Solution propos√©e :**  
Ex√©cuter ruff avec `--fix` pour corriger automatiquement.

```bash
ruff check --select I001 --fix .
```

---

## MOYENNE-028 : Logging f-string (G004)

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `con_cos.py`, `load_config.py`, etc. |
| **Lignes** | 70 occurrences |

**Code probl√©matique :**
```python
logger.info(f"Downloading file: {cos_key}")
```

**Ce qu'il y a √† corriger :**  
f-strings dans les appels de logging. √âvaluation imm√©diate m√™me si le niveau de log est d√©sactiv√©.

**Impact potentiel :**  
- Performance d√©grad√©e
- Non-conformit√© aux bonnes pratiques

**Solution propos√©e :**  
Utiliser la substitution lazy.

```python
logger.info("Downloading file: %s", cos_key)
```

---

## MOYENNE-029 : Datetime sans timezone (DTZ001/DTZ005)

| Attribut | Valeur |
|----------|--------|
| **Scripts** | `batch.py:131`, `logging_utils.py:56,215,261`, `preprocessing_reboot.py:36` |
| **Lignes** | 47 occurrences |

**Code probl√©matique :**
```python
now = datetime.now()
```

**Ce qu'il y a √† corriger :**  
`datetime.now()` sans timezone explicite. Ambigu√Øt√© en production multi-r√©gion.

**Impact potentiel :**  
- Bugs subtils entre timezones
- Logs avec horodatage ambigu

**Solution propos√©e :**  
Sp√©cifier une timezone explicite.

```python
from datetime import datetime, timezone
now = datetime.now(timezone.utc)
```

---

## MOYENNE-030 : os.path au lieu de pathlib (PTH*)

| Attribut | Valeur |
|----------|--------|
| **Scripts** | Multiples |
| **Lignes** | 95 occurrences |

**Code probl√©matique :**
```python
path = os.path.join(PROJECT_ROOT, "config", "app.yml")
```

**Ce qu'il y a √† corriger :**  
95 occurrences de `os.path` au lieu de `pathlib.Path`.

**Impact potentiel :**  
- Code moins moderne
- Manipulation de chemins verbeuse

**Solution propos√©e :**  
Migrer vers pathlib.

```python
from pathlib import Path
path = Path(PROJECT_ROOT) / "config" / "app.yml"
```

---

## MOYENNE-031 : Fonctions trop longues (>50 lignes)

| Attribut | Valeur |
|----------|--------|
| **Scripts** | 18 fonctions |
| **Exemple** | `preprocessing_df_main.py:4` - `df_encoding()` = 319 lignes |

**Ce qu'il y a √† corriger :**  
18 fonctions d√©passent 50 lignes. La plus longue (`df_encoding`) fait 319 lignes.

**Impact potentiel :**  
- Tests unitaires difficiles
- Compr√©hension difficile
- Risque de bugs √©lev√©

**Solution propos√©e :**  
D√©composer en sous-fonctions de 20-30 lignes.

```python
def df_encoding(df: pl.DataFrame) -> pl.DataFrame:
    df = _encode_sector_codes(df)
    df = _encode_legal_nature(df)
    return df
```

---

## MOYENNE-032 : Complexit√© cyclomatique √©lev√©e (CC=20)

| Attribut | Valeur |
|----------|--------|
| **Script** | `config_helper/generate_project_config.py` |
| **Ligne** | 58 |

**Code probl√©matique :**
```python
def generate_process_mode_yaml(config, mode):
    if mode == "batch":
        if config.get("schedule"):
            # ... 20+ branches imbriqu√©es
```

**Ce qu'il y a √† corriger :**  
La fonction a une complexit√© cyclomatique de 20 (limite: 10).

**Impact potentiel :**  
- Difficile √† tester exhaustivement
- Risque √©lev√© de bugs

**Solution propos√©e :**  
D√©composer en fonctions sp√©cialis√©es par mode.

```python
def generate_process_mode_yaml(config: dict, mode: str) -> dict:
    handlers = {"batch": _gen_batch, "stream": _gen_stream}
    return handlers[mode](config)
```

---

## MOYENNE-033 : Fichiers Black non conformes

| Attribut | Valeur |
|----------|--------|
| **Scripts** | 7 fichiers |

**Ce qu'il y a √† corriger :**  
7 fichiers ne sont pas conformes au formatage Black.

**Impact potentiel :**  
- √âchec des hooks pre-commit
- Incoh√©rence de formatage

**Solution propos√©e :**  
Ex√©cuter Black.

```bash
black --line-length 120 .
```

---

## MOYENNE-034 : Acc√®s index sans v√©rification

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/calcul_pdo.py` |
| **Lignes** | 268-269 |

**Code probl√©matique :**
```python
coeffs = dict(zip(feature_order, model.coef_[0]))
intercept = model.intercept_[0]
```

**Ce qu'il y a √† corriger :**  
L'acc√®s `model.coef_[0]` et `model.intercept_[0]` suppose que le mod√®le de r√©gression logistique a bien des coefficients. Si le mod√®le est mal charg√© (fichier pickle corrompu) ou incompatible, ces acc√®s peuvent lever une `IndexError`.

**Impact potentiel :**  
- Crash du batch si le mod√®le est corrompu
- Pas de message d'erreur explicite

**Solution propos√©e :**  
Valider la structure du mod√®le avant utilisation.

```python
if not hasattr(model, 'coef_') or model.coef_.shape[0] == 0:
    raise ValueError("Model has no coefficients - may be corrupted")

coeffs = dict(zip(feature_order, model.coef_[0]))
intercept = model.intercept_[0]
```

---

# üü¢ ISSUES BASSE PRIORIT√â

---

## BASSE-035 : Import inutilis√©

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/con_starburst.py` |
| **Ligne** | 1 |

**Code probl√©matique :**
```python
from __future__ import annotations
```

**Ce qu'il y a √† corriger :**  
Import non utilis√©.

**Solution propos√©e :**  
Supprimer l'import.

---

## BASSE-036 : return None explicite inutile

| Attribut | Valeur |
|----------|--------|
| **Script** | `common/base_transformation.py` |
| **Ligne** | 253 |

**Code probl√©matique :**
```python
def save_data_for_validation(self, df_dict: dict) -> None:
    ...
    return None  # Inutile
```

**Ce qu'il y a √† corriger :**  
`return None` explicite superflu.

**Solution propos√©e :**  
Supprimer le `return None`.

---

# üìã POINTS POSITIFS

‚úÖ Architecture modulaire claire (common/, config/, industrialisation/)  
‚úÖ S√©paration preprocessing par domaine fonctionnel  
‚úÖ Tests unitaires pr√©sents (~8000 lignes)  
‚úÖ Logging structur√© avec StepTracker  
‚úÖ Context managers utilis√©s (StarburstConnector)  
‚úÖ Configuration par environnement (dev/pprod/prod)  
‚úÖ Documentation pr√©sente (README, docs/)  
‚úÖ Pre-commit hooks configur√©s (Black, Ruff, Mypy)  

---

# üìä M√âTRIQUES CIBLES

| M√©trique | Actuel | Cible |
|----------|--------|-------|
| Issues critiques | 8 | 0 |
| Issues haute priorit√© | 11 | 0 |
| Issues moyenne priorit√© | 15 | 0 |
| Erreurs Ruff | 1631 | 0 |
| Erreurs Mypy | 74 | 0 |
| Fichiers Black | 7 | 0 |
| Fonctions sans docstring | 10 | 0 |
| Fonctions >50 lignes | 18 | 0 |
| Max CC | 20 | ‚â§10 |
| Tests int√©gration | 0 | ‚â•10 |
| Secrets en clair | 3 | 0 |
| Divisions non prot√©g√©es | 8 | 0 |
| Gestion NaN/Inf | 0 | 100% |

---

# üöÄ PLAN DE REM√âDIATION

## Phase 0 : IMM√âDIAT (J+0) - S√©curit√© Critique

| Action | Fichier | Effort |
|--------|---------|--------|
| R√©voquer token Artifactory | - | 10 min |
| Nettoyer historique Git (BFG) | - | 30 min |
| Ajouter services_*.env au .gitignore | `_gitignore` | 5 min |

## Phase 1 : URGENT (J+1) - Runtime Errors

| Action | Fichier | Effort |
|--------|---------|--------|
| Prot√©ger divisions c_duree_excce_conso | `preprocessing_safir_conso.py` | 30 min |
| Renforcer protection c_duree_excce_soc | `preprocessing_safir_soc.py` | 30 min |
| Valider SQL injection | `retrieve_sql_query_transac.py` | 1h |
| Fix app.sh stream.py | `app.sh` | 5 min |
| Ajouter validation NaN avant ML | `calcul_pdo.py` | 1h |

## Phase 2 : COURT TERME (J+7) - Qualit√©

| Action | Fichier | Effort |
|--------|---------|--------|
| `ruff check --fix .` | Tous | 10 min |
| `black .` | Tous | 5 min |
| Corriger types Mypy | 23 fichiers | 2h |
| Regrouper window functions | `retrieve_sql_query_transac.py` | 1h |

## Phase 3 : MOYEN TERME (J+30) - Maintenabilit√©

| Action | Fichier | Effort |
|--------|---------|--------|
| Refactorer lazy evaluation | `base_transformation.py` | 3-5j |
| Externaliser codes sectoriels | YAML | 2h |
| Externaliser seuils ML | YAML | 1h |
| Ajouter tests int√©gration | `tests/int√©gration/` | 3j |
| D√©composer fonctions >50 lignes | 18 fonctions | 5j |

---

**Signature Tech Lead :** ________________  
**Date :** 27/01/2026  
**Version :** 7.0 - Rapport Complet Fusionn√©
